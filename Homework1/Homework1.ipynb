{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import mindspore.dataset as ds\n",
    "import mindspore.nn as nn\n",
    "from mindspore.nn.loss import SoftmaxCrossEntropyWithLogits\n",
    "from mindspore.nn.metrics import Accuracy\n",
    "from mindspore import Model\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Dataset Loading"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.9 3.  1.4 0.2 0. ]\n",
      " [4.7 3.2 1.3 0.2 0. ]\n",
      " [4.6 3.1 1.5 0.2 0. ]\n",
      " [5.  3.6 1.4 0.2 0. ]\n",
      " [5.4 3.9 1.7 0.4 0. ]\n",
      " [4.6 3.4 1.4 0.3 0. ]\n",
      " [5.  3.4 1.5 0.2 0. ]\n",
      " [4.9 3.1 1.5 0.2 0. ]\n",
      " [4.4 2.9 1.4 0.1 0. ]\n",
      " [5.4 3.7 1.5 0.2 0. ]\n",
      " [4.8 3.4 1.6 0.2 0. ]\n",
      " [4.8 3.  1.4 0.1 0. ]\n",
      " [4.3 3.  1.1 0.1 0. ]\n",
      " [5.8 4.  1.2 0.2 0. ]\n",
      " [5.7 4.4 1.5 0.4 0. ]\n",
      " [5.4 3.9 1.3 0.4 0. ]\n",
      " [5.1 3.5 1.4 0.3 0. ]\n",
      " [5.7 3.8 1.7 0.3 0. ]\n",
      " [5.1 3.8 1.5 0.3 0. ]\n",
      " [5.4 3.4 1.7 0.2 0. ]\n",
      " [5.1 3.7 1.5 0.4 0. ]\n",
      " [4.6 3.6 1.  0.2 0. ]\n",
      " [5.1 3.3 1.7 0.5 0. ]\n",
      " [4.8 3.4 1.9 0.2 0. ]\n",
      " [5.  3.  1.6 0.2 0. ]\n",
      " [5.  3.4 1.6 0.4 0. ]\n",
      " [5.2 3.5 1.5 0.2 0. ]\n",
      " [5.2 3.4 1.4 0.2 0. ]\n",
      " [4.7 3.2 1.6 0.2 0. ]\n",
      " [4.8 3.1 1.6 0.2 0. ]\n",
      " [5.4 3.4 1.5 0.4 0. ]\n",
      " [5.2 4.1 1.5 0.1 0. ]\n",
      " [5.5 4.2 1.4 0.2 0. ]\n",
      " [4.9 3.1 1.5 0.1 0. ]\n",
      " [5.  3.2 1.2 0.2 0. ]\n",
      " [5.5 3.5 1.3 0.2 0. ]\n",
      " [4.9 3.1 1.5 0.1 0. ]\n",
      " [4.4 3.  1.3 0.2 0. ]\n",
      " [5.1 3.4 1.5 0.2 0. ]\n",
      " [5.  3.5 1.3 0.3 0. ]\n",
      " [4.5 2.3 1.3 0.3 0. ]\n",
      " [4.4 3.2 1.3 0.2 0. ]\n",
      " [5.  3.5 1.6 0.6 0. ]\n",
      " [5.1 3.8 1.9 0.4 0. ]\n",
      " [4.8 3.  1.4 0.3 0. ]\n",
      " [5.1 3.8 1.6 0.2 0. ]\n",
      " [4.6 3.2 1.4 0.2 0. ]\n",
      " [5.3 3.7 1.5 0.2 0. ]\n",
      " [5.  3.3 1.4 0.2 0. ]\n",
      " [7.  3.2 4.7 1.4 1. ]\n",
      " [6.4 3.2 4.5 1.5 1. ]\n",
      " [6.9 3.1 4.9 1.5 1. ]\n",
      " [5.5 2.3 4.  1.3 1. ]\n",
      " [6.5 2.8 4.6 1.5 1. ]\n",
      " [5.7 2.8 4.5 1.3 1. ]\n",
      " [6.3 3.3 4.7 1.6 1. ]\n",
      " [4.9 2.4 3.3 1.  1. ]\n",
      " [6.6 2.9 4.6 1.3 1. ]\n",
      " [5.2 2.7 3.9 1.4 1. ]\n",
      " [5.  2.  3.5 1.  1. ]\n",
      " [5.9 3.  4.2 1.5 1. ]\n",
      " [6.  2.2 4.  1.  1. ]\n",
      " [6.1 2.9 4.7 1.4 1. ]\n",
      " [5.6 2.9 3.6 1.3 1. ]\n",
      " [6.7 3.1 4.4 1.4 1. ]\n",
      " [5.6 3.  4.5 1.5 1. ]\n",
      " [5.8 2.7 4.1 1.  1. ]\n",
      " [6.2 2.2 4.5 1.5 1. ]\n",
      " [5.6 2.5 3.9 1.1 1. ]\n",
      " [5.9 3.2 4.8 1.8 1. ]\n",
      " [6.1 2.8 4.  1.3 1. ]\n",
      " [6.3 2.5 4.9 1.5 1. ]\n",
      " [6.1 2.8 4.7 1.2 1. ]\n",
      " [6.4 2.9 4.3 1.3 1. ]\n",
      " [6.6 3.  4.4 1.4 1. ]\n",
      " [6.8 2.8 4.8 1.4 1. ]\n",
      " [6.7 3.  5.  1.7 1. ]\n",
      " [6.  2.9 4.5 1.5 1. ]\n",
      " [5.7 2.6 3.5 1.  1. ]\n",
      " [5.5 2.4 3.8 1.1 1. ]\n",
      " [5.5 2.4 3.7 1.  1. ]\n",
      " [5.8 2.7 3.9 1.2 1. ]\n",
      " [6.  2.7 5.1 1.6 1. ]\n",
      " [5.4 3.  4.5 1.5 1. ]\n",
      " [6.  3.4 4.5 1.6 1. ]\n",
      " [6.7 3.1 4.7 1.5 1. ]\n",
      " [6.3 2.3 4.4 1.3 1. ]\n",
      " [5.6 3.  4.1 1.3 1. ]\n",
      " [5.5 2.5 4.  1.3 1. ]\n",
      " [5.5 2.6 4.4 1.2 1. ]\n",
      " [6.1 3.  4.6 1.4 1. ]\n",
      " [5.8 2.6 4.  1.2 1. ]\n",
      " [5.  2.3 3.3 1.  1. ]\n",
      " [5.6 2.7 4.2 1.3 1. ]\n",
      " [5.7 3.  4.2 1.2 1. ]\n",
      " [5.7 2.9 4.2 1.3 1. ]\n",
      " [6.2 2.9 4.3 1.3 1. ]\n",
      " [5.1 2.5 3.  1.1 1. ]\n",
      " [5.7 2.8 4.1 1.3 1. ]\n",
      " [6.3 3.3 6.  2.5 2. ]\n",
      " [5.8 2.7 5.1 1.9 2. ]\n",
      " [7.1 3.  5.9 2.1 2. ]\n",
      " [6.3 2.9 5.6 1.8 2. ]\n",
      " [6.5 3.  5.8 2.2 2. ]\n",
      " [7.6 3.  6.6 2.1 2. ]\n",
      " [4.9 2.5 4.5 1.7 2. ]\n",
      " [7.3 2.9 6.3 1.8 2. ]\n",
      " [6.7 2.5 5.8 1.8 2. ]\n",
      " [7.2 3.6 6.1 2.5 2. ]\n",
      " [6.5 3.2 5.1 2.  2. ]\n",
      " [6.4 2.7 5.3 1.9 2. ]\n",
      " [6.8 3.  5.5 2.1 2. ]\n",
      " [5.7 2.5 5.  2.  2. ]\n",
      " [5.8 2.8 5.1 2.4 2. ]\n",
      " [6.4 3.2 5.3 2.3 2. ]\n",
      " [6.5 3.  5.5 1.8 2. ]\n",
      " [7.7 3.8 6.7 2.2 2. ]\n",
      " [7.7 2.6 6.9 2.3 2. ]\n",
      " [6.  2.2 5.  1.5 2. ]\n",
      " [6.9 3.2 5.7 2.3 2. ]\n",
      " [5.6 2.8 4.9 2.  2. ]\n",
      " [7.7 2.8 6.7 2.  2. ]\n",
      " [6.3 2.7 4.9 1.8 2. ]\n",
      " [6.7 3.3 5.7 2.1 2. ]\n",
      " [7.2 3.2 6.  1.8 2. ]\n",
      " [6.2 2.8 4.8 1.8 2. ]\n",
      " [6.1 3.  4.9 1.8 2. ]\n",
      " [6.4 2.8 5.6 2.1 2. ]\n",
      " [7.2 3.  5.8 1.6 2. ]\n",
      " [7.4 2.8 6.1 1.9 2. ]\n",
      " [7.9 3.8 6.4 2.  2. ]\n",
      " [6.4 2.8 5.6 2.2 2. ]\n",
      " [6.3 2.8 5.1 1.5 2. ]\n",
      " [6.1 2.6 5.6 1.4 2. ]\n",
      " [7.7 3.  6.1 2.3 2. ]\n",
      " [6.3 3.4 5.6 2.4 2. ]\n",
      " [6.4 3.1 5.5 1.8 2. ]\n",
      " [6.  3.  4.8 1.8 2. ]\n",
      " [6.9 3.1 5.4 2.1 2. ]\n",
      " [6.7 3.1 5.6 2.4 2. ]\n",
      " [6.9 3.1 5.1 2.3 2. ]\n",
      " [5.8 2.7 5.1 1.9 2. ]\n",
      " [6.8 3.2 5.9 2.3 2. ]\n",
      " [6.7 3.3 5.7 2.5 2. ]\n",
      " [6.7 3.  5.2 2.3 2. ]\n",
      " [6.3 2.5 5.  1.9 2. ]\n",
      " [6.5 3.  5.2 2.  2. ]\n",
      " [6.2 3.4 5.4 2.3 2. ]\n",
      " [5.9 3.  5.1 1.8 2. ]]\n"
     ]
    }
   ],
   "source": [
    "from numpy import genfromtxt\n",
    "\n",
    "iris_data = genfromtxt('iris.csv', delimiter=',')\n",
    "print(iris_data[2:])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# data preprocessing\n",
    "iris_data = iris_data[1:]\n",
    "X = iris_data[:, :4].astype(np.float32)\n",
    "y = iris_data[:, -1].astype(np.int32)\n",
    "\n",
    "# data normalization\n",
    "X /= np.max(np.abs(X), axis=0)\n",
    "\n",
    "# split into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "train_data = (X_train, y_train)\n",
    "train_data = ds.NumpySlicesDataset(train_data)\n",
    "\n",
    "test_data = (X_test, y_test)\n",
    "test_data = ds.NumpySlicesDataset(test_data)\n",
    "\n",
    "train_data = train_data.batch(32)\n",
    "test_data = test_data.batch(32)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Network Definition"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "class my_net(nn.Cell):\n",
    "    def __init__(self):\n",
    "        super(my_net, self).__init__()\n",
    "        self.fc1 = nn.Dense(4, 10)\n",
    "        self.fc2 = nn.Dense(10, 3)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def construct(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model Training"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] ME(1400484:139886717112704,MainProcess):2022-03-15-21:58:02.939.80 [mindspore/train/model.py:536] The CPU cannot support dataset sink mode currently.So the training process will be performed with dataset not sink.\n",
      "[WARNING] ME(1400484:139886717112704,MainProcess):2022-03-15-21:58:02.995.352 [mindspore/train/model.py:954] CPU cannot support dataset sink mode currently.So the evaluating process will be performed with dataset non-sink mode.\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'accuracy': 0.9666666666666667}"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = my_net()\n",
    "\n",
    "net_loss = SoftmaxCrossEntropyWithLogits(sparse=True)\n",
    "learning_rate = 0.01\n",
    "momentum = 0.9\n",
    "net_opt = nn.Momentum(net.trainable_params(), learning_rate, momentum)\n",
    "\n",
    "model = Model(net, net_loss, net_opt, metrics={\"accuracy\": Accuracy()})\n",
    "\n",
    "model.train(10, train_data)\n",
    "model.eval(test_data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Quiz"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "\"\\nQuestions:\\n\\n1. Why should the if usage =='train' judgment formula be added before data\\nargumentation?\\n2. What are the data augmentation methods? Give examples.\\n3. What is a step?\\n4. What base class should I inherit when writing a user-defined callback function?\\n5. How can I obtain important information (loss function, optimizer, and current epoch\\nnumber) during the training process in the user-defined callback function?\\n6. If a model in this experiment is underfitting, you can optimize the model by increasing\\nthe network complexity. If a model is overfitting, which method can be used to solve the\\nproblem?\\n7. What is the dataset sink mode?\\n8. Why does the model's loss value in this experiment fluctuate greatly in the training set\\nand is higher than that in the test set?\\n\""
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Questions:\n",
    "\n",
    "1. When iris_data is processed, why is the data in the last column changed to be in the int form?\n",
    "2. What is an epoch?\n",
    "3. In this experiment, the SoftmaxCrossEntropyWithLogits(sparse=True) loss function is\n",
    "used. Why is sparse set to True?\n",
    "4. What are the application scenarios of SoftmaxCrossEntropyWithLogits and SoftmaxCrossEntropy?\n",
    "5. This experiment is a classification task. If a regression task is performed, what loss functions can be set?\n",
    "6. If a regression task is performed, what is the number of output channels?\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "'\\nAnswers:\\n\\n1. Because it is label column, and need to be transformed in the same type as the rest data.\\n2. The number of cycles of training the network\\n3. \"Set this parameter to True if the output is not one-hot encoding\" -> which means labeled/categorized data, and some models cannot operate on labels properly, that is why it\\'s required to convert labeled data into integer type.\\n4.\\n5.\\n6.\\n'"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Answers:\n",
    "\n",
    "1. Because it is a label column, it must be transformed in the integer type as the rest of the data.\n",
    "2. The number of cycles of training the network.\n",
    "3. \"Set this parameter to True if the output is not one-hot encoding\" -> This means labeled/categorized data, and some models cannot properly operate on labels, which is why labeled data must be converted to integer type.\n",
    "Ex:\n",
    "red,\tgreen,\tblue\n",
    "1,\t\t0,\t\t0\n",
    "0,\t\t1,\t\t0\n",
    "0,\t\t0,\t\t1\n",
    "\n",
    "4. Classification\n",
    "5. Mean Squared Error, Mean Squared Logarithmic Error, Mean Absolute Error, I guess.\n",
    "6. -\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MindSpore(1.5)",
   "language": "python",
   "name": "mindspore"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}